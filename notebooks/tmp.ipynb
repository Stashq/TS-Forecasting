{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/stachu/Projects/Anomaly_detection/TSAD')\n",
    "# import sys\n",
    "# sys.path.append(\"/home/stachu/Projects/Anomaly_detection/TSAD\")\n",
    "\n",
    "from predpy.dataset import MultiTimeSeriesDataset\n",
    "from predpy.data_module import MultiTimeSeriesModule\n",
    "from predpy.wrapper import Autoencoder, Predictor, VAE\n",
    "from predpy.experimentator import (\n",
    "    DatasetParams, ModelParams,\n",
    "    Experimentator, load_experimentator, load_last_experimentator)\n",
    "from predpy.plotter import (\n",
    "    plot_exp_predictions\n",
    ")\n",
    "from predpy.preprocessing import set_index\n",
    "from predpy.preprocessing import moving_average\n",
    "from predpy.preprocessing import (\n",
    "    load_and_preprocess, set_index, moving_average, drop_if_is_in,\n",
    "    use_dataframe_func, loc, iloc, get_isoforest_filter, get_variance_filter)\n",
    "from predpy.trainer import (\n",
    "    CheckpointParams, TrainerParams, EarlyStoppingParams, LoggerParams)\n",
    "from predpy.experimentator import LearningParams\n",
    "from tsad.noiser import apply_noise_on_dataframes, white_noise\n",
    "from tsad.anomaly_detector import PredictionAnomalyDetector, ReconstructionAnomalyDetector\n",
    "from models import LSTMAE, LSTMVAE\n",
    "from literature.anom_trans import AnomalyTransformer, ATWrapper\n",
    "from literature.velc import VELC, VELCWrapper\n",
    "from literature.dagmm import DAGMM, DAGMMWrapper\n",
    "from literature.tadgan import TADGAN, TADGANWrapper\n",
    "from literature.anomaly_detector_base import AnomalyDetector\n",
    "from models.ideas import LSTMMVR, ConvMVR, MVRWrapper\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.base import TransformerMixin\n",
    "# from tsai.models import TCN, ResNet, TST, RNN, TransformerModel, FCN\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "from typing import List, Dict, Literal\n",
    "from predpy.plotter import plot_anomalies\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 200\n",
    "batch_size = 64\n",
    "\n",
    "c_in = 38\n",
    "c_out = 38\n",
    "topic = \"Industry\"\n",
    "collection_name = \"ServerMachineDataset\"\n",
    "dataset_name1 = \"machine-1-1\"\n",
    "dataset_name2 = \"machine-1-2\"\n",
    "dataset_name3 = \"machine-1-3\"\n",
    "\n",
    "# c_in = 1\n",
    "# c_out = 1\n",
    "# topic = \"Handmade\"\n",
    "# collection_name = \"Sin\"\n",
    "# dataset_name = \"artificial_1\"\n",
    "\n",
    "load_params = {\n",
    "    \"header\": None, \"names\": [str(i) for i in range(c_in)]\n",
    "}\n",
    "\n",
    "drop_refill_pipeline = []\n",
    "preprocessing_pipeline = [\n",
    "    (use_dataframe_func, \"astype\", \"float\"),\n",
    "]\n",
    "detect_anomalies_pipeline = []\n",
    "\n",
    "datasets_params = [\n",
    "    DatasetParams(\n",
    "        path=\"/home/stachu/Projects/Anomaly_detection/TSAD/data/%s/%s/train/%s.csv\" % (topic, collection_name, dataset_name1),\n",
    "        load_params=load_params,\n",
    "        target=[str(i) for i in range(c_in)],\n",
    "        split_proportions=[0.8, 0.1, 0.1],\n",
    "        window_size=window_size,\n",
    "        batch_size=batch_size,\n",
    "        drop_refill_pipeline=drop_refill_pipeline,\n",
    "        preprocessing_pipeline=preprocessing_pipeline,\n",
    "        detect_anomalies_pipeline=detect_anomalies_pipeline,\n",
    "        scaler=StandardScaler()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_params = [\n",
    "    ModelParams(\n",
    "        name_=f'ConvMVR_ws{window_size}_nk10_ks3_es50', cls_=ConvMVR,\n",
    "        init_params=dict(\n",
    "            window_size=window_size, c_in=c_in, n_kernels=10,\n",
    "            kernel_size=3, emb_size=50, lambda_=0.3),\n",
    "        WrapperCls=MVRWrapper\n",
    "    ),\n",
    "    ModelParams(\n",
    "        name_=f'ConvMVR_ws{window_size}_nk10_ks3_es50', cls_=ConvMVR,\n",
    "        init_params=dict(\n",
    "            window_size=window_size, c_in=c_in, n_kernels=10,\n",
    "            kernel_size=3, emb_size=50, lambda_=0.5),\n",
    "        WrapperCls=MVRWrapper\n",
    "    ),\n",
    "    ModelParams(\n",
    "        name_=f'ConvMVR_ws{window_size}_nk10_ks3_es50', cls_=ConvMVR,\n",
    "        init_params=dict(\n",
    "            window_size=window_size, c_in=c_in, n_kernels=10,\n",
    "            kernel_size=3, emb_size=50, lambda_=0.7),\n",
    "        WrapperCls=MVRWrapper\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "chp_p = CheckpointParams(\n",
    "    dirpath=\"./checkpoints\", monitor='val_loss', verbose=True,\n",
    "    save_top_k=1)\n",
    "tr_p = TrainerParams(\n",
    "    max_epochs=30, gpus=1, auto_lr_find=False)\n",
    "es_p = EarlyStoppingParams(\n",
    "    monitor='val_loss', patience=5, min_delta=1e-2, verbose=True)\n",
    "\n",
    "exp = Experimentator(\n",
    "    models_params=models_params,\n",
    "    datasets_params=datasets_params,\n",
    "    trainer_params=tr_p,\n",
    "    checkpoint_params=chp_p,\n",
    "    early_stopping_params=es_p,\n",
    "    LoggersClasses=[TensorBoardLogger],\n",
    "    loggers_params=[LoggerParams(save_dir=\"./lightning_logs\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = load_experimentator('./saved_experiments/2022-05-30_22:57:20.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "    path: Path, window_size: int, ts_scaler: TransformerMixin = None\n",
    ") -> MultiTimeSeriesDataset:\n",
    "    df = pd.read_csv(\n",
    "        path, header=None\n",
    "    )\n",
    "    try:\n",
    "        df.columns = df.columns.astype(int)\n",
    "    except TypeError:\n",
    "        pass\n",
    "    if ts_scaler is not None:\n",
    "        df[:] = ts_scaler.transform(df)\n",
    "    dataset = MultiTimeSeriesDataset(\n",
    "        sequences=[df],\n",
    "        window_size=window_size,\n",
    "        target=df.columns.tolist()\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def exp_fit_run_detection(exp: Experimentator, min_points: int = 3, if_plot: bool = True):\n",
    "    for ds_id in range(exp.datasets_params.shape[0]):\n",
    "        dataset_name = exp.datasets_params.iloc[ds_id]['name_']\n",
    "        test_cls_path = './data/%s/%s/test_label/%s.csv' % (topic, collection_name, dataset_name)\n",
    "\n",
    "        dataset = get_dataset(\n",
    "            path='./data/%s/%s/test/%s.csv' % (topic, collection_name, dataset_name),\n",
    "            window_size=window_size, ts_scaler=exp.get_targets_scaler(ds_id))\n",
    "        data_classes = pd.read_csv(\n",
    "            test_cls_path, header=None)\\\n",
    "            .iloc[:, 0].to_list()\n",
    "        # rec_classes = None\n",
    "        rec_classes = dataset.get_recs_cls_by_data_cls(\n",
    "            data_classes, min_points=min_points)\n",
    "        n_models = exp.models_params.shape[0]\n",
    "\n",
    "        for m_id in range(0, n_models):\n",
    "            model_name = exp.models_params.iloc[m_id]['name_']\n",
    "            try:\n",
    "                model = exp.load_pl_model(\n",
    "                    m_id, os.path.join('checkpoints', dataset_name, model_name))\n",
    "                model.fit_run_detection(\n",
    "                    window_size=window_size,\n",
    "                    test_path='./data/%s/%s/test/%s.csv' % (topic, collection_name, dataset_name),\n",
    "                    rec_classes=rec_classes,\n",
    "                    test_cls_path=test_cls_path,\n",
    "                    min_points=min_points, scale_scores=True,  # class_weight=class_weight,\n",
    "                    ts_scaler=exp.get_targets_scaler(ds_id),\n",
    "                    # load_scores_path = './saved_scores_preds/%s/%s/%s/anom_scores.csv' % (collection_name, dataset_name, model_name),\n",
    "                    save_scores_path= './saved_scores_preds/%s/%s/%s/anom_scores.csv' % (collection_name, dataset_name, model_name),\n",
    "                    # load_preds_path = './saved_scores_preds/%s/%s/%s/preds.csv' % (collection_name, dataset_name, model_name),\n",
    "                    save_preds_path = './saved_scores_preds/%s/%s/%s/preds.csv' % (collection_name, dataset_name, model_name),\n",
    "                    plot=if_plot,  # start_plot_pos=15000, end_plot_pos=21000,\n",
    "                    save_html_path='./pages/%s/%s/%s.html' % (collection_name, dataset_name, model_name),\n",
    "                    f_score_beta=0.5,\n",
    "                    wdd_t_max=window_size/2,\n",
    "                    wdd_w_f=0.001,\n",
    "                    wdd_ma_f=0.001\n",
    "                )\n",
    "            except Exception as e:\n",
    "                # raise e\n",
    "                print('Problem with fit_run_detection on model \"%s\".' % model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stachu/.cache/pypoetry/virtualenvs/tsad-8szBw8Wl-py3.8/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "\n",
      "Collecting record classes: 100%|██████████| 28279/28279 [00:24<00:00, 1132.61it/s]\n",
      "/home/stachu/.cache/pypoetry/virtualenvs/tsad-8szBw8Wl-py3.8/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "\n",
      "Calculating dataset anomaly scores: 100%|██████████| 442/442 [00:21<00:00, 20.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best class weights: {0.6, 0.4}\n",
      "[[24219   200]\n",
      " [ 1968  1892]]\n",
      "Problem with fit_run_detection on model \"ConvMVR_ws200_nk10_ks3_es50\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stachu/.cache/pypoetry/virtualenvs/tsad-8szBw8Wl-py3.8/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "\n",
      "Calculating dataset anomaly scores:  57%|█████▋    | 254/442 [00:13<00:10, 18.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb Cell 6'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000005?line=0'>1</a>\u001b[0m \u001b[39m# exp.datasets_params['path'].apply(lambda x: x[-20:])\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000005?line=1'>2</a>\u001b[0m \u001b[39m# exp.models_params['name_']\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000005?line=2'>3</a>\u001b[0m exp_fit_run_detection(exp, min_points\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, if_plot\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;32m/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb Cell 5'\u001b[0m in \u001b[0;36mexp_fit_run_detection\u001b[0;34m(exp, min_points, if_plot)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=38'>39</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=39'>40</a>\u001b[0m     model \u001b[39m=\u001b[39m exp\u001b[39m.\u001b[39mload_pl_model(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=40'>41</a>\u001b[0m         m_id, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mcheckpoints\u001b[39m\u001b[39m'\u001b[39m, dataset_name, model_name))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=41'>42</a>\u001b[0m     model\u001b[39m.\u001b[39;49mfit_run_detection(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=42'>43</a>\u001b[0m         window_size\u001b[39m=\u001b[39;49mwindow_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=43'>44</a>\u001b[0m         test_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./data/\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m/test/\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m.csv\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m%\u001b[39;49m (topic, collection_name, dataset_name),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=44'>45</a>\u001b[0m         rec_classes\u001b[39m=\u001b[39;49mrec_classes,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=45'>46</a>\u001b[0m         test_cls_path\u001b[39m=\u001b[39;49mtest_cls_path,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=46'>47</a>\u001b[0m         min_points\u001b[39m=\u001b[39;49mmin_points, scale_scores\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  \u001b[39m# class_weight=class_weight,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=47'>48</a>\u001b[0m         ts_scaler\u001b[39m=\u001b[39;49mexp\u001b[39m.\u001b[39;49mget_targets_scaler(ds_id),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=48'>49</a>\u001b[0m         \u001b[39m# load_scores_path = './saved_scores_preds/%s/%s/%s/anom_scores.csv' % (collection_name, dataset_name, model_name),\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=49'>50</a>\u001b[0m         save_scores_path\u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m./saved_scores_preds/\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m/anom_scores.csv\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m%\u001b[39;49m (collection_name, dataset_name, model_name),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=50'>51</a>\u001b[0m         \u001b[39m# load_preds_path = './saved_scores_preds/%s/%s/%s/preds.csv' % (collection_name, dataset_name, model_name),\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=51'>52</a>\u001b[0m         save_preds_path \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m./saved_scores_preds/\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m/preds.csv\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m%\u001b[39;49m (collection_name, dataset_name, model_name),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=52'>53</a>\u001b[0m         plot\u001b[39m=\u001b[39;49mif_plot,  \u001b[39m# start_plot_pos=15000, end_plot_pos=21000,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=53'>54</a>\u001b[0m         save_html_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./pages/\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m.html\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m%\u001b[39;49m (collection_name, dataset_name, model_name),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=54'>55</a>\u001b[0m         f_score_beta\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=55'>56</a>\u001b[0m         wdd_t_max\u001b[39m=\u001b[39;49mwindow_size\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=56'>57</a>\u001b[0m         wdd_w_f\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=57'>58</a>\u001b[0m         wdd_ma_f\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=58'>59</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=59'>60</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=60'>61</a>\u001b[0m     \u001b[39m# raise e\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stachu/Projects/Anomaly_detection/TSAD/notebooks/tmp.ipynb#ch0000004?line=61'>62</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mProblem with fit_run_detection on model \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m model_name)\n",
      "File \u001b[0;32m~/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py:114\u001b[0m, in \u001b[0;36mAnomalyDetector.fit_run_detection\u001b[0;34m(self, test_path, window_size, min_points, plot, scale_scores, batch_size, save_html_path, test_cls_path, rec_classes, load_scores_path, save_scores_path, load_preds_path, save_preds_path, ts_scaler, start_plot_pos, end_plot_pos, f_score_beta, wdd_t_max, wdd_w_f, wdd_ma_f)\u001b[0m\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=107'>108</a>\u001b[0m     data_classes \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=108'>109</a>\u001b[0m         test_cls_path, header\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\\\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=109'>110</a>\u001b[0m         \u001b[39m.\u001b[39miloc[:, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto_list()\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=110'>111</a>\u001b[0m     rec_classes \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mget_recs_cls_by_data_cls(\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=111'>112</a>\u001b[0m         data_classes, min_points\u001b[39m=\u001b[39mmin_points)\n\u001b[0;32m--> <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=113'>114</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_detector(\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=114'>115</a>\u001b[0m     dataloader\u001b[39m=\u001b[39;49mdataloader,\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=115'>116</a>\u001b[0m     classes\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49marray(rec_classes),\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=116'>117</a>\u001b[0m     plot\u001b[39m=\u001b[39;49mplot, scale_scores\u001b[39m=\u001b[39;49mscale_scores,\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=117'>118</a>\u001b[0m     save_html_path\u001b[39m=\u001b[39;49msave_html_path,\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=118'>119</a>\u001b[0m     \u001b[39m# class_weight=class_weight,\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=119'>120</a>\u001b[0m     load_scores_path\u001b[39m=\u001b[39;49mload_scores_path,\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=120'>121</a>\u001b[0m     save_scores_path\u001b[39m=\u001b[39;49msave_scores_path,\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=121'>122</a>\u001b[0m     load_preds_path\u001b[39m=\u001b[39;49mload_preds_path,\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=122'>123</a>\u001b[0m     save_preds_path\u001b[39m=\u001b[39;49msave_preds_path,\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=123'>124</a>\u001b[0m     ts_scaler\u001b[39m=\u001b[39;49mts_scaler,\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=124'>125</a>\u001b[0m     start_plot_pos\u001b[39m=\u001b[39;49mstart_plot_pos,\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=125'>126</a>\u001b[0m     end_plot_pos\u001b[39m=\u001b[39;49mend_plot_pos,\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=126'>127</a>\u001b[0m     f_score_beta\u001b[39m=\u001b[39;49mf_score_beta,\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=127'>128</a>\u001b[0m     wdd_t_max\u001b[39m=\u001b[39;49mwdd_t_max, wdd_w_f\u001b[39m=\u001b[39;49mwdd_w_f,\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=128'>129</a>\u001b[0m     wdd_ma_f\u001b[39m=\u001b[39;49mwdd_ma_f\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=129'>130</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py:159\u001b[0m, in \u001b[0;36mAnomalyDetector.fit_detector\u001b[0;34m(self, dataloader, classes, load_scores_path, save_scores_path, load_preds_path, save_preds_path, plot, start_plot_pos, end_plot_pos, scale_scores, ts_scaler, save_html_path, f_score_beta, wdd_t_max, wdd_w_f, wdd_ma_f)\u001b[0m\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=156'>157</a>\u001b[0m         preds \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(load_preds_path)\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=157'>158</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=158'>159</a>\u001b[0m     scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscore_dataset(\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=159'>160</a>\u001b[0m         dataloader\u001b[39m=\u001b[39;49mdataloader, scale\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, return_pred\u001b[39m=\u001b[39;49mplot)\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=160'>161</a>\u001b[0m     \u001b[39mif\u001b[39;00m plot:\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=161'>162</a>\u001b[0m         scores, preds \u001b[39m=\u001b[39m scores\n",
      "File \u001b[0;32m~/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py:472\u001b[0m, in \u001b[0;36mAnomalyDetector.score_dataset\u001b[0;34m(self, dataloader, scale, return_pred)\u001b[0m\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=468'>469</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=469'>470</a>\u001b[0m         dataloader, desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCalculating dataset anomaly scores\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=470'>471</a>\u001b[0m     x \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m--> <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=471'>472</a>\u001b[0m     scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49manomaly_score(\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=472'>473</a>\u001b[0m         x, scale\u001b[39m=\u001b[39mscale, return_pred\u001b[39m=\u001b[39mreturn_pred)\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=473'>474</a>\u001b[0m     \u001b[39mif\u001b[39;00m return_pred:\n\u001b[1;32m    <a href='file:///home/stachu/Projects/Anomaly_detection/TSAD/literature/anomaly_detector_base.py?line=474'>475</a>\u001b[0m         scores, x_dash \u001b[39m=\u001b[39m scores\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# exp.datasets_params['path'].apply(lambda x: x[-20:])\n",
    "# exp.models_params['name_']\n",
    "exp_fit_run_detection(exp, min_points=3, if_plot=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0087b2b215c3c30120cea1dc155e9068bf3b8ab33d57dfddd212a0ad2b2b3350"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tsad-8szBw8Wl-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
